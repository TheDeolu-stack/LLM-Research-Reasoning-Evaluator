# LLM-Research-Reasoning-Evaluator
This project evaluates the reasoning, logic, analytical capability, and accuracy of different Large Language Models (LLMs) using a curated set of test cases.


## ğŸ” What This Project Does
- Tests LLMs on reasoning, logic, math, fact-checking, and pattern recognition
- Scores responses using a structured rubric
- Documents failures, edge cases, hallucinations, and reasoning gaps
- Provides a replicable evaluation methodology


## ğŸ“‚ Structure
- test_cases â†’ All reasoning challenges
- evaluations â†’ Results from each LLM
- scoring â†’ Scoring rubric and aggregated scores
- notes â†’ Methodology and evaluation guidelines


## ğŸš€ How to Use
1. Select a model 
2. Feed in test cases one by one
3. Save raw outputs into the evaluations folder
4. Score using rubric 
5. Analyze trends and weaknesses


## ğŸ§  Why This Project Matters
Understanding model strengths and weaknesses prepares you for roles in:
- LLM evaluation
- AI quality assurance
- Prompt engineering
- Research analysis
